\chapter{Badania}
\section{Miary jakości klasyfikatora}
TODO opisać accuracy, precision itp.

\section{Plan badań}
Celem niniejszej pracy naukowej jest sprawdzenie jak istotny wpływ na~dokładność klasyfikacji mają 2~czynniki:
\begin{itemize}
    \item regularyzacja typu L2 (\ref{sssec:reg_L2}),
    \item normalizacja lokalnej odpowiedzi (\ref{sssec:normalizacja_odpowiedzi}).
\end{itemize}

W~artykule ,,Practical Recommendations for Gradient-Based Training of Deep Architectures''
(\cite{practical-gradient-based}) skrótowo omówiono problem doboru hiperparametrów sieci. Jednym ze~sposobów
przedstawionych w~opracowaniu jest określenie wartości brzegowych dla~optymalizowanych hiperparametrów,
a~następnie zbadanie przestrzeni między. Po~zbadaniu zachowania sieci dla~tej przestrzeni hiperparametrów, można podjąć
decyzję o~przyjęciu jednego z~zestawów hiperparametrów dla~sieci lub~rozszerzyć pole poszukiwań o~kolejne obszary.

Dla~parametru regularyzacji~L2 (tzw.~\textbf{weight decay}) jako~górne ograniczenie przyjęto początkowo wartość~0.05.
Wybór bazował na~tym, że~w~podobnych sieciach, tj.~przeznaczonych do~identyfikacji obiektów przedstawianych na~obrazkach
z~bazy ImageNet (\cite{imagenet}), wartość tego hiperparametru nie~przekraczała~0.03. Badanie miało sprawdzić
również jak~sieć zachowywałaby~się~bez regularyzacji wag. Stąd jako dolne ograniczenie przyjęto wartość~0.

Dla~parametru decydującego o~wpływie normalizacji lokalnej odpowiedzi (tzw.~parametr $\alpha$) jako ograniczenie górne
przyjęto początkowo wartość~0.001, a~jako ograniczenie dolne wartość~0. Usprawiedliwienie dla~tych decyzji było
takie samo, jak~dla~wyborów dokonanych przy~hiperparametrze regularyzacji~L2.

Dla~każdej pary parametrów sieć była uczona 100,000 mini-zestawów danych (tzw.~\textbf{minibatch}), z~których
każdy zawierał 128 przykładów uczących. Po~każdym kroku uczenia pojedynczym mini-zestawem danych, sprawdzano
dokładność sieci na~zbiorze testowym. Po~wykonaniu wszystkich kroków uczenia dla~danej pary hiperparametrów
brano średnią dokładność sieci na~zbiorze testowym ze~100 ostatnich kroków uczenia. Wyniki przedstawiono w~tabelce
(\ref{table:wyniki1}).

\begin{table}
    \centering
    \begin{tabular}{|l|l|l|l|l|l|}
      \hline
                       & wd = 0.0005 & wd = 0.001 & wd = 0.005 & wd = 0.01 & wd = 0.05 \\
      \hline
      $\alpha=0.00001$ & 0.83 & 0.79 & 0.76 & 0.75 & 0.78 \\
      \hline
      $\alpha=0.00005$ & 0.82 & 0.80 & 0.77 & 0.78 & 0.75 \\
      \hline
      $\alpha=0.0001$  & 0.84 & 0.81 & 0.80 & 0.73 & 0.77 \\
      \hline
      $\alpha=0.0005$  & 0.85 & 0.83 & 0.84 & 0.81 & 0.79 \\
      \hline
      $\alpha=0.001$   & 0.84 & 0.84 & 0.82 & 0.78 & 0.76 \\
      \hline
    \end{tabular}
    \caption{Wpływ regularyzacji L2 ($wd$) i~normalizacji lokalnego kontrastu ($\alpha$) na~dokładność klasyfikacji sieci
    neuronowej}
    \label{table:wyniki1}
\end{table}

Całkowity czas badania wyniósł: 40 godzin 10 minut i 58 sekund.

\section{Środowisko sprzętowe}
Badania zostały wykonane z wykorzystaniem następującego zestawu komputerowego:
\begin{itemize}
    \item \textbf{procesor}~--~Intel Core i7-4771 3,5Ghz (8 rdzeni),
    \item \textbf{płyta główna}~--~TODO,
    \item \textbf{karta graficzna}~--~MSI GeForce GTX 780 Ti,
    \item \textbf{pamięć RAM}~--~TODO.
\end{itemize}

\section{Omówienie wyników badań}
Wyniki uzysakne w~badaniu wskazują, że~wzraz ze~wzrostem wpływu regularyzacji~L2 na~sieć neuronową, dokładność
klasyfikacji ulegała pogorszeniu. Jednocześnie najlepsze wyniki osiągane były dla~wartości $\alpha$ w~okolicach 0.0005.

W~przypadku wpływu normalizacji lokalnego kontrastu wyniki są~zgodne z~oczekiwaniami, tj.~dla~odpowiednio dobranych
parametrów normalizacja ta~przynosi poprawę rezultatów. Sprzeczne z~przewidywaniami okazały się~zmiany wartości
dokładności dla~różnych wartości parametru $wd$. Spodziewano się, że~regularyzacja zapobiegając overfittingowi polepszy
dokładność klasyfikacji.

Wytłumaczeniem takiego stanu rzeczy może być brak wystąpienia zjawiska overfittingu, z~dwóch powodów:
\begin{itemize}
    \item zbyt mała liczba neuronów w~sieci, a~przez~to~niska szansa na~zbytnie dopasowanie się~sieci do~danych
          wejściowych (overfitting),
    \item zbyt niska liczba iteracji podczas uczenia sieci.
\end{itemize}
Regularyzacja~L2 choć~zapobiega wystąpieniu overfittingu, to~jednak ma~negatywny wpływ na~szybkość uczenia się~sieci,
przez~co~potrzebna jest większa liczba iteracji w~procesie uczenia, aby~osiągnąć zadowalający poziom dokładności.

TODO badanie dla większej liczby iteracji (lepsze) lub dla większej liczby neuronów.
