\chapter{Architektura sieci neuronowej}
Podczas tworzenia splotowej sieci neuronowej, należy dobrać wiele metaparametrów takich jak:
\begin{itemize}
    \item liczba warstw splotowych,
    \item liczba jąder stosowanych do~wykonywania splotów w~każdej z~warstw splotowych,
    \item rozkład warstw typu max-pooling (\ref{sec:inferencja}),
    \item rozkład warstw normalizujących (\ref{sssec:normalizacja}),
    \item współczynnik uczenia (\ref{ssec:backpropagation}),
    \item współczynnik zmniejszania współczynnika uczenia (\ref{sssec:alpha-decay}).
\end{itemize}

Ogólne zasady dotyczące pierwszych dwóch z~wymienionych punktów zostały opisane w~artykule
,,Rethinking the Inception Architecture for Computer Vision''(\cite{RIACV}). Posiłkując się~przytoczoną pracą można
wymienić kilka wskazówek przydatnych przy~ustalaniu metaparametrów sieci:
\begin{itemize}
    \item unikanie zbyt małej liczby neuronów w~warstwach, w~sczególności w~warstwach początkowych. Warto zastosować
          kilkukrotność/kilkunastokrotność spodziewanej liczby klas, które ma~rozpoznawać sieć
          (w~przypadku CIFAR-10 jest to 10 klas). Warstwy końcowe mogą zawierać mniejszą liczbę neuronów, niż warstwy
          poprzednie.
    \item zmniejszenie rozmiaru danych wejściowych poprzez zastosowanie metod, takich jak:
          \begin{itemize}
              \item usunięcie brzegów, gdyż~zwykle zawierają mało istotne dane,
              \item zmniejszenie rozmiaru obrazka poprzez zastosowanie sklaowania.
          \end{itemize}
    \item używanie niewielkich filtrów splotowych (np. 3x3 lub 5x5 zamiast 7x7). Lepsze efekty daje zastosowanie dwóch
          warstw splotowych o~maskach 3x3 niż jednej maski 7x7,
    \item warto zacząć od 2 do 5 warstw splotowych (tyle samo warstw skalujących i~normalizujących), następnie zwiększać
          liczbę masek używanych w~warstwach splotowych na przemian ze~zwiększaniem liczby warstw.
\end{itemize}

\subsection{Architektura badanej sieci}
Badana sieć w swojej podstawowej wersji bazuje na~architekturze AlexNet przedstawionej w~artykule \cite{AlexNet}.
Po dokonaniu drobnych modyfikacji w~końcowych etapach przetwarzania obrazu, sieć składa się~z~następujących warstw:
\begin{enumerate}
    \item Warstwy splotowej z~64 maskami o rozmiarze 5x5x3 (wysokość x szerokość x liczba objętych kanałów).
          Maska przesuwana jest zawsze o~1~piksel (w~kierunku pionowym i~poziomym).
    \item Warstwy skalującej typu max-pooling o~wielkości filtra 3x3x1 (wysokość x szerokość x liczba objętych kanałów).
          Filtr jest przesuwany o~2~piksele (w~kierunku pionowym i~poziomym)
    \item Warstwy normalizującej (lokalna normalizacja odpowiedzi).
    \item Warstwy splotowej z~64 maskami o rozmiarze 5x5x64 (wysokość x szerokość x liczba objętych kanałów).
          Maska przesuwana jest zawsze o~1 piksel (niezależnie od~kierunku przesuwania maski).
    \item Warstwy normalizującej (lokalna normalizacja odpowiedzi).
    \item Warstwy skalującej typu max-pooling o~wielkości filtra 3x3x1 (wysokość x szerokość x liczba objętych kanałów).
          Filtr jest przesuwany o~2~piksele (w~kierunku pionowym i~poziomym).
    \item Warstwy w~pełni połączonej (standardowa warstwa w~sieciach neuronowych) z~384 neuronami i~funkcją aktywacji
          typu ReLU.
    \item Warstwy w~pełni połączonej z~192 neuronami i~funkcją aktywacji
          typu ReLU.
    \item Warstwy wyjściowej (również w~pełni połączonej) z~10 neuronami (tyle samo, co~klas do~rozpoznawania).
          Warstwa wyjściowa zawiera funkcję aktywacji typu softmax.
\end{enumerate}

TODO opisać że badany będzie weight decay i rozmiar batcha (artykuł od Maćka)
% jakie wartości dla weight decay
% jakie wartości dla alpha decay (są algorytmy optymalizacji które same to estymują).
% ustawienie seeda do generatora liczb losowych w tensorflow -- tf.set_random_seed(1)
%   http://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed
\chapter{Badania}

\section{Plan badań}
% sieć bez ulepszeń
% sieć z weight decay
% sieć z learning rate decay
% sieć ze weight decay i learning rate decay

\section{Środowisko sprzętowe}
Wszystkie badania zostały wykonane z wykorzystaniem następującego zestawu komputerowego:
\begin{itemize}
    \item \textbf{procesor}~--~TODO,
    \item \textbf{płyta główna}~--~TODO,
    \item \textbf{karta graficzna}~--~TODO,
    \item \textbf{pamięć RAM}~--~TODO.
\end{itemize}

\section{Wyniki badań}
TODO
% accuracy na zbiorze ewaluacyjnym
%    dla różnych weight decay
%    dla różnych learning rate decay
%    dla sieci z weight decay i learning rate decay (razem) o najlepszych parametrach spośród badanych
% accuracy na zbiorze ewaluacyjnym i testowym dla sieci w finalnej wersji (weight decay i learning rate decay)

\subsection{Omówienie wyników}
TODO
% dlaczego weight decay pomaga
% dlaczego learning rate decay pomaga
